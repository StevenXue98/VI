{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzHaU1mC9kUj"
   },
   "source": [
    "## What is JAX?\n",
    "\n",
    "[JAX](https://github.com/google/jax) = [autograd](https://github.com/HIPS/autograd) + [XLA](https://www.tensorflow.org/xla) = Numpy + Autodiff + XLA(Accelerated Linear Algebra)\n",
    "\n",
    "JAX is a Python library which augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. Concretely, this makes it simple to write standard Python/numpy code and immediately be able to\n",
    "\n",
    "\n",
    "*   Compute the derivative of a function via a successor to autograd (`jax.grad`)\n",
    "*   Compile and run your numpy programs on GPUs and TPUs via XLA by default. Just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API (`jax.jit`)\n",
    "*   Automagically vectorize a function, so that e.g. you can process a “batch” of data in parallel (`jax.vmap/jax.pmap`)\n",
    "\n",
    "\n",
    "**Disclaimer: I'm by no mean an expert in JAX.**\n",
    "- \"A good programmer is typically the one who is good at googling\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJclSXaAAhnT"
   },
   "source": [
    "## Why JAX?\n",
    "* Clean and unified API\n",
    "* Powerful functional transformations\n",
    "* Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X97PSE4o9QbI",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:23:10.760385Z",
     "start_time": "2025-04-03T19:23:10.290361Z"
    }
   },
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L0GnQq92FKKM",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:26:39.454160Z",
     "start_time": "2025-04-03T19:26:39.448768Z"
    }
   },
   "source": [
    "def predict(params, inputs):\n",
    "  for W, b in params:\n",
    "    outputs = np.dot(inputs, W) + b\n",
    "    inputs = np.tanh(outputs)\n",
    "  return outputs\n",
    "\n",
    "def logprob_fun(params, inputs, targets):\n",
    "  preds = predict(params, inputs)\n",
    "  return np.sum((preds - targets)**2)\n",
    "\n",
    "grad_fun = jax.jit(jax.grad(logprob_fun))  # compiled gradient evaluation function\n",
    "perex_grads = jax.jit(jax.vmap(grad_fun, in_axes=(None, 0, 0)))  # fast per-example grads"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU-W78QGC4s4"
   },
   "source": [
    "## Example (credit to Colin Raffel)\n",
    "\n",
    "We will be learning the XOR function with a small neural network. The XOR function takes as input two binary numbers and outputs a binary number, like so:\n",
    "\n",
    "In 1 | In 2 | Out\n",
    "---- | ---- | ---\n",
    "0    | 0    | 0   \n",
    "0    | 1    | 1  \n",
    "1    | 0    | 1   \n",
    "1    | 1    | 0  \n",
    "\n",
    "We'll use a neural network with a single hidden layer with 3 neurons and a hyperbolic tangent nonlinearity, trained with the cross-entropy loss via stochastic gradient descent. Let's implement this model and loss function. Note that the code is exactly as you'd write in standard `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2d3ym8eDFPPm",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:29:25.980086Z",
     "start_time": "2025-04-03T19:29:25.973810Z"
    }
   },
   "source": [
    "# Sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Computes our network's output\n",
    "def net(params, x):\n",
    "    w1, b1, w2, b2 = params\n",
    "    hidden = np.tanh(np.dot(w1, x) + b1)\n",
    "    return sigmoid(np.dot(w2, hidden) + b2)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def loss(params, x, y):\n",
    "    out = net(params, x)\n",
    "    cross_entropy = -y * np.log(out) - (1 - y)*np.log(1 - out)\n",
    "    return cross_entropy\n",
    "\n",
    "# Utility function for testing whether the net produces the correct\n",
    "# output for all possible inputs\n",
    "def test_all_inputs(inputs, params):\n",
    "    predictions = [int(net(params, inp) > 0.5) for inp in inputs]\n",
    "    for inp, out in zip(inputs, predictions):\n",
    "        print(inp, '->', out)\n",
    "    return (predictions == [onp.bitwise_xor(*inp) for inp in inputs])"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzNZ3plEFikK"
   },
   "source": [
    "There are some places where we want to use standard numpy rather than jax.numpy. One of those places is with parameter initialization. We'd like to initialize our parameters randomly before we train our network, which is not an operation for which we need derivatives or compilation. JAX uses its own jax.random library instead of numpy.random which provides better support for reproducibility (seeding) across different transformations. Since we don't need to transform the initialization of parameters in any way, it's simplest just to use standard numpy.random instead of jax.random here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JxKiA8oiFsKK",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:31:46.991779Z",
     "start_time": "2025-04-03T19:31:46.988067Z"
    }
   },
   "source": [
    "def initial_params():\n",
    "    return [\n",
    "        onp.random.randn(3, 2),  # w1\n",
    "        onp.random.randn(3),  # b1\n",
    "        onp.random.randn(3),  # w2\n",
    "        onp.random.randn(),  #b2\n",
    "    ]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iME_ETxGF4VK"
   },
   "source": [
    "## `jax.grad`\n",
    "\n",
    "The first transformation we'll use is `jax.grad`. `jax.grad` takes a function and returns a new function which computes the gradient of the original function. By default, the gradient is taken with respect to the first argument; this can be controlled via the `argnums` argument to `jax.grad`. To use gradient descent, we want to be able to compute the gradient of our loss function with respect to our neural network's parameters. For this, we'll simply use `jax.grad(loss)` which will give us a function we can call to get these gradients."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qn-Ya7Z9F_Xb",
    "outputId": "c5cb2872-5e7b-4f27-c31e-2ec4764456e7",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:43:25.617084Z",
     "start_time": "2025-04-03T19:43:24.410908Z"
    }
   },
   "source": [
    "loss_grad = jax.grad(loss)\n",
    "\n",
    "# Stochastic gradient descent learning rate\n",
    "learning_rate = 1.\n",
    "# All possible inputs\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Initialize parameters randomly\n",
    "params = initial_params()\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Grab a single random input\n",
    "    x = inputs[onp.random.choice(inputs.shape[0])]\n",
    "    # Compute the target output\n",
    "    y = onp.bitwise_xor(*x)\n",
    "    # Get the gradient of the loss for this input/output pair\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Update parameters via gradient descent\n",
    "    params = [param - learning_rate * grad\n",
    "              for param, grad in zip(params, grads)]\n",
    "    # Every 100 iterations, check whether we've solved XOR\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 1\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_QBYIU4GN-v"
   },
   "source": [
    "## `jax.jit`\n",
    "\n",
    "While carefully-written `numpy` code can be reasonably performant, for modern machine learning we want our code to run as fast as possible. JAX provides a JIT (just-in-time) compiler which takes a standard Python/`numpy` function and compiles it to run efficiently on an accelerator. Compiling a function also avoids the overhead of the Python interpreter, which helps whether or not you're using an accelerator. In total, `jax.jit` can dramatically speed-up your code with essentially no coding overhead - you just ask JAX to compile the function for you. Even our tiny neural network can see a pretty dramatic speedup when using `jax.jit`:\n",
    "\n",
    "**Warning**: not all functions can be jit'ed (see [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-+-JIT))."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaxZ5HGfGeZ8",
    "outputId": "d9583f56-d902-422a-f49f-db9209efc17d",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:44:56.176351Z",
     "start_time": "2025-04-03T19:44:46.975207Z"
    }
   },
   "source": [
    "# Time the original gradient function\n",
    "%timeit loss_grad(params, x, y)\n",
    "loss_grad = jax.jit(jax.grad(loss))\n",
    "# Run once to trigger JIT compilation\n",
    "loss_grad(params, x, y)\n",
    "%timeit loss_grad(params, x, y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.27 ms ± 35.1 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "9.01 μs ± 44.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sDbSs2bGsON"
   },
   "source": [
    "## `jax.vmap`\n",
    "\n",
    "An astute reader may have noticed that we have been training our neural network on a single example at a time. This is \"true\" stochastic gradient descent; in practice, when training modern machine learning models we perform \"minibatch\" gradient descent where we average the loss gradients over a mini-batch of examples at each step of gradient descent. JAX provides `jax.vmap`, which is a transformation which automatically \"vectorizes\" a function. What this means is that it allows you to compute the output of a function in parallel over some axis of the input. For us, this means we can apply the `jax.vmap` function transformation and immediately get a version of our loss function gradient which is amenable to using a minibatch of examples.\n",
    "\n",
    "`jax.vmap` takes in additional arguments:\n",
    "- `in_axes` is a tuple or integer which tells JAX over which axes the function's arguments should be parallelized. The tuple should have the same length as the number of arguments of the function being `vmap`'d, or should be an integer when there is only one argument. In our example, we'll use `(None, 0, 0)`, meaning \"don't parallelize over the first argument (`params`), and parallelize over the first (zeroth) dimension of the second and third arguments (`x` and `y`)\".\n",
    "- `out_axes` is analogous to `in_axes`, except it specifies which axes of the function's output to parallelize over. In our case, we'll use `0`, meaning to parallelize over the first (zeroth) dimension of the function's sole output (the loss gradients).\n",
    "\n",
    "Note that we will have to change the training code a little bit - we need to grab a batch of data instead of a single example at a time, and we need to average the gradients over the batch before applying them to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZC9PAJmdHEC9",
    "outputId": "d6fdb8a9-386f-48eb-a5c8-702454d931b1",
    "ExecuteTime": {
     "end_time": "2025-04-03T19:53:25.004155Z",
     "start_time": "2025-04-03T19:53:24.499686Z"
    }
   },
   "source": [
    "loss_grad = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0), out_axes=0))\n",
    "\n",
    "params = initial_params()\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for n in itertools.count():\n",
    "    # Generate a batch of inputs\n",
    "    x = inputs[onp.random.choice(inputs.shape[0], size=batch_size)]\n",
    "    y = onp.bitwise_xor(x[:, 0], x[:, 1])\n",
    "    # The call to loss_grad remains the same!\n",
    "    grads = loss_grad(params, x, y)\n",
    "    # Note that we now need to average gradients over the batch\n",
    "    params = [param - learning_rate * np.mean(grad, axis=0)\n",
    "              for param, grad in zip(params, grads)]\n",
    "    if not n % 100:\n",
    "        print('Iteration {}'.format(n))\n",
    "        if test_all_inputs(inputs, params):\n",
    "            break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "[0 0] -> 1\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 1\n",
      "Iteration 100\n",
      "[0 0] -> 0\n",
      "[0 1] -> 1\n",
      "[1 0] -> 1\n",
      "[1 1] -> 0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6njY4t9NIVSY"
   },
   "source": [
    "## SPMD programming with `jax.pmap`\n",
    "\n",
    "For parallel programming of multiple accelerators, like multiple GPUs, use pmap. With pmap you write single-program multiple-data (SPMD) programs, including fast parallel collective communication operations. See MNIST [example](https://github.com/google/jax/blob/master/examples/spmd_mnist_classifier_fromscratch.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coGrLeQxinPA"
   },
   "source": [
    "## Neural Network Libraries\n",
    "\n",
    "There are several neural net libraries built on top of JAX. Depending what you're trying to do, you have several options:\n",
    "\n",
    "- For toy functions and simple architectures (e.g. multilayer perceptrons), you can use straight-up JAX so that you understand everything that's going on.\n",
    "- `Stax` is a very lightweight neural net package with easy-to-follow source code. It's good for implementing simpler architectures like CIFAR conv nets, and has the advantage that you can understand the whole control flow of the code.\n",
    "- There are various full-featured deep learning frameworks built on top of JAX and designed to resemble other frameworks you might be familiar with, such as `PyTorch` or `Keras`. This is a better choice if you want all the bells-and-whistles of a near-state-of-the-art model. The main choices are [`Flax`](https://github.com/google/flax), [`Haiku`](https://github.com/deepmind/dm-haiku), and [`Objax`](https://github.com/google/objax), and the choice between them might come down to which ones already have a public implementation of something you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZgUNPScgzhw",
    "outputId": "7068709d-fe94-450a-adbd-e1dd0854b336"
   },
   "source": [
    "!pip install objax\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import objax\n",
    "from objax.zoo.wide_resnet import WideResNet"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting objax\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/5a/e0/54503c60e3a04c23600dd9cf099c5adfda7c76d1240fd3bff4e71e3164b7/objax-1.2.0.tar.gz (41kB)\n",
      "\r\u001B[K     |████████                        | 10kB 24.4MB/s eta 0:00:01\r\u001B[K     |███████████████▉                | 20kB 30.8MB/s eta 0:00:01\r\u001B[K     |███████████████████████▉        | 30kB 23.7MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▊| 40kB 18.6MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 51kB 7.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from objax) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.6/dist-packages (from objax) (1.19.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from objax) (7.0.0)\n",
      "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from objax) (0.1.57+cuda101)\n",
      "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from objax) (0.2.7)\n",
      "Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from objax) (2.4.0)\n",
      "Collecting parameterized\n",
      "  Downloading https://files.pythonhosted.org/packages/31/13/fe468c8c7400a8eca204e6e160a29bf7dcd45a76e20f1c030f3eaa690d93/parameterized-0.8.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/dist-packages (from jaxlib->objax) (1.12)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib->objax) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->objax) (3.3.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (1.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (1.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (0.4.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (51.1.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (3.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (0.36.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.3.0->objax) (1.32.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (4.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.3.0->objax) (3.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax) (1.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.3.0->objax) (3.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3.0->objax) (0.4.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.3.0->objax) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.3.0->objax) (3.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3.0->objax) (3.1.0)\n",
      "Building wheels for collected packages: objax\n",
      "  Building wheel for objax (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for objax: filename=objax-1.2.0-cp36-none-any.whl size=65294 sha256=c994aa3985a43f9432d7f7e1b6ef3b8d302956aa7177638db7b8351a7a7c1fd4\n",
      "  Stored in directory: /root/.cache/pip/wheels/8e/97/e4/20367226fc4a9eafd3f0a4e9b557920d5b18920296c0eb18c7\n",
      "Successfully built objax\n",
      "Installing collected packages: parameterized, objax\n",
      "Successfully installed objax-1.2.0 parameterized-0.8.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3L5MuwXRgwil",
    "outputId": "ea98bf8d-597a-424a-e612-94278e107f76"
   },
   "source": [
    "# Data\n",
    "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "X_train = X_train.transpose(0, 3, 1, 2) / 255.0\n",
    "X_test = X_test.transpose(0, 3, 1, 2) / 255.0\n",
    "\n",
    "# Model\n",
    "model = WideResNet(nin=3, nclass=10, depth=28, width=2)\n",
    "opt = objax.optimizer.Adam(model.vars())\n",
    "\n",
    "# Losses\n",
    "@objax.Function.with_vars(model.vars())\n",
    "def loss(x, label):\n",
    "    logit = model(x, training=True)\n",
    "    return objax.functional.loss.cross_entropy_logits_sparse(logit, label).mean()\n",
    "\n",
    "gv = objax.GradValues(loss, model.vars())\n",
    "\n",
    "@objax.Function.with_vars(model.vars() + opt.vars())\n",
    "def train_op(x, y, lr):\n",
    "    g, v = gv(x, y)\n",
    "    opt(lr=lr, grads=g)\n",
    "    return v\n",
    "\n",
    "\n",
    "train_op = objax.Jit(train_op)\n",
    "predict = objax.Jit(objax.nn.Sequential([\n",
    "    objax.ForceArgs(model, training=False), objax.functional.softmax\n",
    "]))\n",
    "\n",
    "\n",
    "def augment(x):\n",
    "    if random.random() < .5:\n",
    "        x = x[:, :, :, ::-1]  # Flip the batch images about the horizontal axis\n",
    "    # Pixel-shift all images in the batch by up to 4 pixels in any direction.\n",
    "    x_pad = np.pad(x, [[0, 0], [0, 0], [4, 4], [4, 4]], 'reflect')\n",
    "    rx, ry = np.random.randint(0, 8), np.random.randint(0, 8)\n",
    "    x = x_pad[:, :, rx:rx + 32, ry:ry + 32]\n",
    "    return x\n",
    "\n",
    "\n",
    "# Training\n",
    "# print(model.vars())\n",
    "for epoch in range(30):\n",
    "    # Train\n",
    "    loss = []\n",
    "    sel = np.arange(len(X_train))\n",
    "    np.random.shuffle(sel)\n",
    "    for it in range(0, X_train.shape[0], 64):\n",
    "        loss.append(train_op(augment(X_train[sel[it:it + 64]]), Y_train[sel[it:it + 64]].flatten(),\n",
    "                             4e-3 if epoch < 20 else 4e-4))\n",
    "\n",
    "    # Eval\n",
    "    test_predictions = [predict(x_batch).argmax(1) for x_batch in X_test.reshape((50, -1) + X_test.shape[1:])]\n",
    "    accuracy = np.array(test_predictions).flatten() == Y_test.flatten()\n",
    "    print(f'Epoch {epoch + 1:4d}  Loss {np.mean(loss):.2f}  Accuracy {100 * np.mean(accuracy):.2f}')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n",
      "Epoch    1  Loss 1.53  Accuracy 58.20\n",
      "Epoch    2  Loss 1.06  Accuracy 63.95\n",
      "Epoch    3  Loss 0.85  Accuracy 69.92\n",
      "Epoch    4  Loss 0.70  Accuracy 74.81\n",
      "Epoch    5  Loss 0.60  Accuracy 74.73\n",
      "Epoch    6  Loss 0.53  Accuracy 80.52\n",
      "Epoch    7  Loss 0.48  Accuracy 82.04\n",
      "Epoch    8  Loss 0.44  Accuracy 82.75\n",
      "Epoch    9  Loss 0.41  Accuracy 85.02\n",
      "Epoch   10  Loss 0.38  Accuracy 85.09\n",
      "Epoch   11  Loss 0.35  Accuracy 86.25\n",
      "Epoch   12  Loss 0.32  Accuracy 87.10\n",
      "Epoch   13  Loss 0.31  Accuracy 86.81\n",
      "Epoch   14  Loss 0.29  Accuracy 84.76\n",
      "Epoch   15  Loss 0.26  Accuracy 86.53\n",
      "Epoch   16  Loss 0.25  Accuracy 88.06\n",
      "Epoch   17  Loss 0.24  Accuracy 88.58\n",
      "Epoch   18  Loss 0.22  Accuracy 88.91\n",
      "Epoch   19  Loss 0.22  Accuracy 89.05\n",
      "Epoch   20  Loss 0.21  Accuracy 88.96\n",
      "Epoch   21  Loss 0.13  Accuracy 91.51\n",
      "Epoch   22  Loss 0.11  Accuracy 91.67\n",
      "Epoch   23  Loss 0.10  Accuracy 91.32\n",
      "Epoch   24  Loss 0.09  Accuracy 91.81\n",
      "Epoch   25  Loss 0.09  Accuracy 91.41\n",
      "Epoch   26  Loss 0.08  Accuracy 91.48\n",
      "Epoch   27  Loss 0.08  Accuracy 91.43\n",
      "Epoch   28  Loss 0.07  Accuracy 91.45\n",
      "Epoch   29  Loss 0.07  Accuracy 91.42\n",
      "Epoch   30  Loss 0.07  Accuracy 91.63\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
