{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q3f-Q0IDE6V"
   },
   "source": [
    "## Recap\n",
    "\n",
    "JAX is a Python library which augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. Concretely, this makes it simple to write standard Python/numpy code and immediately be able to\n",
    "\n",
    "\n",
    "*   Compute the derivative of a function via a successor to autograd (`jax.grad`)\n",
    "*   Compile and run your numpy programs on GPUs and TPUs via XLA by default. Just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API (`jax.jit`)\n",
    "*   Automagically vectorize a function, so that e.g. you can process a “batch” of data in parallel (`jax.vmap/jax.pmap`)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0X8-XaZGC8YN",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:01:18.540022Z",
     "start_time": "2025-04-03T20:01:18.230468Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp\n",
    "\n",
    "from jax import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vj25f_qLYEGY"
   },
   "source": [
    "## Today's focus - [Autodiff](https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf)\n",
    "\n",
    "- `jax.vjp` computes the vector-Jacobian product (aka reverse-mode autodiff) which is the key component of `jax.grad`\n",
    "- `jax.jvp` computes the Jacobian-vector product (aka forward-mode autodiff) which is the key component of `jax.linearize`\n",
    "- Using these two functions for many interesting matrix-vector product (e.g., Hessian-vector product)\n",
    "- Use cases of matrix-vector product in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxuaLVHcaa-x"
   },
   "source": [
    "## `jax.vjp`\n",
    "\n",
    "**A bit about math**\n",
    "\n",
    "Mathematically, suppose we have a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix of $f$ at a particular point $x$, denoted $J(x) \\in \\mathbb{R}^{m \\times n}$, is a matrix:\n",
    "$$J(x) =\n",
    "\\left(\\begin{matrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{matrix} \\right)$$\n",
    "\n",
    "You can think of it as a linear map $J(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ which maps $v$ to $J(x)v$.\n",
    "\n",
    "What vector-Jacobian product does is to compute $vJ(x)$ or $J(x)^\\top v$. `jax.vjp` is the api to compute the vector-Jacobian product in JAX with two arguments:\n",
    "- first argument: a callable function $f$\n",
    "- second argument: primal value at which point the Jacobian is evaluated (Should be either a tuple or a list of arguments)\n",
    "\n",
    "It returns both $f(x)$ and a linear map $J(x)^\\top: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ which map $v$ to $J^\\top v$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dj-XYpdkaO2o",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:05:36.680171Z",
     "start_time": "2025-04-03T20:05:36.350334Z"
    }
   },
   "source": [
    "f = lambda x: np.sum(3 * x ** 2)\n",
    "\n",
    "y, vjp_fun = jax.vjp(f, np.ones((2, 2)))\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key)\n",
    "v = random.normal(subkey, y.shape)\n",
    "\n",
    "# compute J^T v\n",
    "vjp = vjp_fun(v)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfylaVrSeCVP"
   },
   "source": [
    "The `jax.grad` function we used all the time is essentially a vector-Jacobian product."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qcltnAIiaEgx",
    "outputId": "05bb1560-03ae-4f07-85b4-339cafec1483",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:08:10.430302Z",
     "start_time": "2025-04-03T20:08:10.422288Z"
    }
   },
   "source": [
    "def my_grad(f, x):\n",
    "  y, vjp_fn = jax.vjp(f, x)\n",
    "  return vjp_fn(np.ones(y.shape))[0]\n",
    "\n",
    "f = lambda x: np.sum(3 * x ** 2)\n",
    "print(\"my_grad:\\n {}\".format(my_grad(f, np.ones((2, 2)))))\n",
    "print(\"jax grad:\\n {}\".format(jax.grad(f)(np.ones((2, 2)))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_grad:\n",
      " [[6. 6.]\n",
      " [6. 6.]]\n",
      "jax grad:\n",
      " [[6. 6.]\n",
      " [6. 6.]]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYhcXvMDgr8v"
   },
   "source": [
    "## `jax.jvp`\n",
    "\n",
    "What Jacobian-vector product does is to compute $J(x)v$, which is the first-order term in Taylor expansion. Recall the Taylor expansion for a scalar function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$:\n",
    "$$f(x) = f(x_0) + J(x_0)(x - x_0) + o(\\| x - x_0 \\|)$$\n",
    "\n",
    "`jax.jvp` is the api to compute the Jacobian-vector product in JAX with three arguments:\n",
    "- first argument: a callable function $f$\n",
    "- second argument: primal value at which point the Jacobian is evaluated (Should be either a tuple or a list of arguments)\n",
    "- third argument: tangent vector $v$\n",
    "\n",
    "It returns both $f(x)$ and $J(x)v$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdnF-903i0WS",
    "outputId": "6df814fa-fa72-48b9-d4c2-7646de02116f",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:10:51.105318Z",
     "start_time": "2025-04-03T20:10:51.046534Z"
    }
   },
   "source": [
    "f = lambda x: np.sum(3 * x ** 2)\n",
    "primal_value = np.ones((2, 2))\n",
    "tangent_vector = np.ones((2, 2))\n",
    "y, f_jvp = jax.jvp(f, (primal_value, ), (tangent_vector, ))\n",
    "print(y, f_jvp)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 24.0\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hzzra8Fjkaq"
   },
   "source": [
    "### There is another function `jax.linearize` basically doing the same thing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5Rr5y-7j1G2",
    "outputId": "906eb1e9-b4b6-4fdb-afd3-acca2854158e",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:11:04.720786Z",
     "start_time": "2025-04-03T20:11:04.677786Z"
    }
   },
   "source": [
    "y, f_linearized = jax.linearize(f, primal_value)\n",
    "f_jvp_ = f_linearized(tangent_vector)\n",
    "print(y, f_jvp_)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 24.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuc1edI1mM1M"
   },
   "source": [
    "### Implementing `jax.jvp` using two `jax.vjp` (see this [thread](https://github.com/renmengye/tensorflow-forward-ad/issues/2))\n",
    "\n",
    "- One important fact: for a linear operator $f(x) = Ax$, `vjp` returns $A^\\top x$.\n",
    "- In the first `vjp`, we get $J^\\top v_\\text{dummy}$ which is a linear operator with $A = J^\\top$. In this step, we could use any $v_\\text{dummy}$.\n",
    "- In the second `vjp`, we get $Jv$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVX9kxFCmUzV",
    "outputId": "ba962a36-d5bf-4bcc-80e8-02f886072578",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:13:05.708560Z",
     "start_time": "2025-04-03T20:13:05.635570Z"
    }
   },
   "source": [
    "def my_jvp(f, x, v):\n",
    "  ans, f_vjp = jax.vjp(f, x) # f_vjp returns a linear map J^T\n",
    "  dummy_var = np.zeros_like(ans)\n",
    "  _, f_vjp_vjp = jax.vjp(f_vjp, dummy_var) # f_vjp_vjp returns J\n",
    "  return f_vjp_vjp((v,))[0]\n",
    "\n",
    "f_jvp__ = my_jvp(f, primal_value, tangent_vector)\n",
    "print(f_jvp__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w51BplVrqODp",
    "outputId": "984878eb-9ef6-4605-efbe-e697c62871de",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:13:32.058377Z",
     "start_time": "2025-04-03T20:13:16.041661Z"
    }
   },
   "source": [
    "%timeit jax.jvp(f, (primal_value, ), (tangent_vector, ))\n",
    "%timeit f_linearized(tangent_vector)\n",
    "%timeit my_jvp(f, primal_value, tangent_vector)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 μs ± 3.02 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "64.4 μs ± 135 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "1.07 ms ± 1.78 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCWW5d-8kg89"
   },
   "source": [
    "## Matrix-vector product\n",
    "\n",
    "- Hessian-vector product $H(x)v$\n",
    "- Gauss-Newton Hessian-vector product $J(x)^\\top J(x)v$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BExrNo_NshV3"
   },
   "source": [
    "### Hessian-vector product\n",
    "In many cases, the Hessian matrix is extremely large, we could compute or store it. Fortunately, Hessian-vector product is typically enough for many things in machine learning.\n",
    "- Computes $H^{-1}v$ in Newton method or influence function\n",
    "- Computes the top eigenvalues of $H$ using power iteration or [Lanczos algorithm](https://arxiv.org/abs/1901.10159)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ti-6F_k6uZvY",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:17:32.444443Z",
     "start_time": "2025-04-03T20:17:32.440755Z"
    }
   },
   "source": [
    "def hvp(f, x, v):\n",
    "  return jax.grad(lambda x: np.vdot(jax.grad(f)(x), v))(x)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK-_tvL9uvUM"
   },
   "source": [
    "That’s efficient, but we can do even better and save some memory by using forward-mode together with reverse-mode.\n",
    "\n",
    "$$H(x) v = \\partial^2 f(x) v = \\partial g(x) v$$\n",
    "\n",
    "All we need is applying `JVP` on gradient function $g$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MetM6U2pvKkp",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:17:40.013471Z",
     "start_time": "2025-04-03T20:17:40.010780Z"
    }
   },
   "source": [
    "def new_hvp(f, x, v):\n",
    "  return jax.jvp(jax.grad(f), (x,), (v,))[1]"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4720gUKbuuR9",
    "outputId": "3007a9f0-0219-4aee-9665-ca308405e4fd",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:17:46.688725Z",
     "start_time": "2025-04-03T20:17:46.511221Z"
    }
   },
   "source": [
    "f = lambda x: np.sum(3 * x ** 2)\n",
    "primal_value = np.ones((2, 2))\n",
    "tangent_vector = np.ones((2, 2))\n",
    "\n",
    "print(hvp(f, primal_value, tangent_vector))\n",
    "print(new_hvp(f, primal_value, tangent_vector))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6.]\n",
      " [6. 6.]]\n",
      "[[6. 6.]\n",
      " [6. 6.]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfPEilTnx5nl",
    "outputId": "87491b6b-8692-4ca6-a652-d94a18d3ee63",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:18:11.849544Z",
     "start_time": "2025-04-03T20:17:49.069856Z"
    }
   },
   "source": [
    "%timeit hvp(f, primal_value, tangent_vector)\n",
    "%timeit new_hvp(f, primal_value, tangent_vector)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.87 ms ± 65.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "938 μs ± 1.32 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xw9X7yvJyIn9"
   },
   "source": [
    "### Gauss-Newton Hessian-vector product\n",
    "\n",
    "$J(x)^\\top J(x)v$ Can be done by a `JVP` and a `VJP`. In training neural networks, we typically prefer Gauss-Newton matrix or Fisher matrix because they are positive semi-definite.\n",
    "\n",
    "For training neural networks, we could use Gauss-newton updates:\n",
    "$$ \\theta_{t+1} = \\theta_t - \\eta G^{-1}g_t$$\n",
    "To compute $G^{-1}g_t$, we solve the linear system $Gx = g_t$ using [conjugate gradient](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf).\n",
    "\n",
    "**Caveat**: $G$ needs to be positive definite since conjugate gradient solves the linear system by minimizing $\\frac{1}{2}x^\\top G x - b x$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AWFjprTXJzeS",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:22:17.807768Z",
     "start_time": "2025-04-03T20:22:17.802184Z"
    }
   },
   "source": [
    "def conjugate_gradient(mvp, b, damping=0.01, max_iter=30):\n",
    "  x = np.zeros_like(b)\n",
    "  r = b\n",
    "  p = r\n",
    "  rdotr = r.dot(r)\n",
    "  for i in range(1, max_iter):\n",
    "    Ap = mvp(p) + damping * p\n",
    "    v = rdotr / p.dot(Ap)\n",
    "    x += v * p\n",
    "    r -= v * Ap\n",
    "    newrdotr = r.dot(r)\n",
    "    mu = newrdotr / rdotr\n",
    "    p = r + mu * p\n",
    "    rdotr = newrdotr\n",
    "  return x"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0wWbvdpJ0Cqo",
    "ExecuteTime": {
     "end_time": "2025-04-03T20:22:28.655888Z",
     "start_time": "2025-04-03T20:22:28.400370Z"
    }
   },
   "source": [
    "import time\n",
    "import itertools\n",
    "import scipy\n",
    "import numpy.random as npr\n",
    "\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "from jax import jit, grad, random\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import Dense, Tanh, LogSoftmax\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from keras.datasets import mnist\n",
    "\n",
    "rng = random.PRNGKey(0)\n",
    "\n",
    "step_size = 0.1 # 0.003 for SGD, 0.3 for PSGD\n",
    "num_epochs = 10\n",
    "batch_size = 5000\n",
    "\n",
    "\n",
    "def loss(params, batch):\n",
    "  inputs, targets = batch\n",
    "  preds = predict(params, inputs)\n",
    "  one_hot_targets = jax.nn.one_hot(targets, 10)\n",
    "  return np.mean(np.sum((preds - one_hot_targets) ** 2, axis=1))\n",
    "\n",
    "def accuracy(params, batch):\n",
    "  inputs, targets = batch\n",
    "  predicted_class = np.argmax(predict(params, inputs), axis=1)\n",
    "  return np.mean(predicted_class == targets)\n",
    "\n",
    "init_random_params, predict = stax.serial(\n",
    "    Dense(512), Tanh,\n",
    "    Dense(512), Tanh,\n",
    "    Dense(10))\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1, 28 * 28)\n",
    "test_images = test_images.reshape(-1, 28 * 28)\n",
    "\n",
    "num_train = train_images.shape[0]\n",
    "num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "num_batches = num_complete_batches + bool(leftover)\n",
    "\n",
    "def data_stream():\n",
    "  rng = npr.RandomState(0)\n",
    "  while True:\n",
    "    perm = rng.permutation(num_train)\n",
    "    for i in range(num_batches):\n",
    "      batch_idx = perm[i * batch_size:(i + 1) * batch_size]\n",
    "      yield train_images[batch_idx], train_labels[batch_idx]\n",
    "batches = data_stream()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'optimizers' from 'jax.experimental' (/Users/stevenxue/anaconda3/envs/vi/lib/python3.13/site-packages/jax/experimental/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jit, grad, random\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizers\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stax\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mjax\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstax\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dense, Tanh, LogSoftmax\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'optimizers' from 'jax.experimental' (/Users/stevenxue/anaconda3/envs/vi/lib/python3.13/site-packages/jax/experimental/__init__.py)"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qw2NU9w8_wJv"
   },
   "source": [
    "opt_init, opt_update, get_params = optimizers.sgd(step_size)\n",
    "\n",
    "@jit\n",
    "def update(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "\n",
    "def gvp(params, v, batch):\n",
    "  inputs, _ = batch\n",
    "  f = lambda p: predict(p, inputs)\n",
    "  _, unravel_fn = ravel_pytree(params)\n",
    "  jvp = jax.jvp(f, [params], [unravel_fn(v)])[1]\n",
    "  _, f_vjp = jax.vjp(f, params)\n",
    "  gvp = f_vjp(jvp)[0]\n",
    "  return ravel_pytree(gvp)[0] / inputs.shape[0]\n",
    "\n",
    "@jit\n",
    "def precon_update(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  grads = grad(loss)(params, batch)\n",
    "  # solve linear system using conjugate gradient\n",
    "  _, unravel_fn = ravel_pytree(params)\n",
    "  mvp = lambda v: gvp(params, v, batch)\n",
    "  precon_grads = conjugate_gradient(mvp, ravel_pytree(grads)[0])\n",
    "  precon_grads = unravel_fn(precon_grads)\n",
    "  return opt_update(i, precon_grads, opt_state)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eV7xct48_yW2",
    "outputId": "33bf455e-69ca-4c10-f6dc-072df4fa2d35"
   },
   "source": [
    "_, init_params = init_random_params(rng, (-1, 28 * 28))\n",
    "opt_state = opt_init(init_params)\n",
    "itercount = itertools.count()\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "  start_time = time.time()\n",
    "  for _ in range(num_batches):\n",
    "    # opt_state = update(next(itercount), opt_state, next(batches))\n",
    "    opt_state = precon_update(next(itercount), opt_state, next(batches))\n",
    "  epoch_time = time.time() - start_time\n",
    "\n",
    "  params = get_params(opt_state)\n",
    "  train_acc = accuracy(params, (train_images, train_labels))\n",
    "  test_acc = accuracy(params, (test_images, test_labels))\n",
    "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
    "  print(\"Training set accuracy {}\".format(train_acc))\n",
    "  print(\"Test set accuracy {}\".format(test_acc))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 0 in 17.62 sec\n",
      "Training set accuracy 0.7765666842460632\n",
      "Test set accuracy 0.7611000537872314\n",
      "Epoch 1 in 1.75 sec\n",
      "Training set accuracy 0.913266658782959\n",
      "Test set accuracy 0.8947000503540039\n",
      "Epoch 2 in 1.75 sec\n",
      "Training set accuracy 0.9402833580970764\n",
      "Test set accuracy 0.9181000590324402\n",
      "Epoch 3 in 1.77 sec\n",
      "Training set accuracy 0.9518333673477173\n",
      "Test set accuracy 0.9320000410079956\n",
      "Epoch 4 in 1.79 sec\n",
      "Training set accuracy 0.9576333165168762\n",
      "Test set accuracy 0.9379000663757324\n",
      "Epoch 5 in 1.80 sec\n",
      "Training set accuracy 0.9619166851043701\n",
      "Test set accuracy 0.9403000473976135\n",
      "Epoch 6 in 1.81 sec\n",
      "Training set accuracy 0.9637166857719421\n",
      "Test set accuracy 0.9419000744819641\n",
      "Epoch 7 in 1.82 sec\n",
      "Training set accuracy 0.9664999842643738\n",
      "Test set accuracy 0.9429000616073608\n",
      "Epoch 8 in 1.84 sec\n",
      "Training set accuracy 0.9680500030517578\n",
      "Test set accuracy 0.9442000389099121\n",
      "Epoch 9 in 1.85 sec\n",
      "Training set accuracy 0.9689000248908997\n",
      "Test set accuracy 0.9445000290870667\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}
